{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/miniconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/miniconda3/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/miniconda3/lib/python3.12/site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/trentyoung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/trentyoung/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/trentyoung/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /opt/miniconda3/lib/python3.12/site-packages (0.11.7)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in /opt/miniconda3/lib/python3.12/site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in /opt/miniconda3/lib/python3.12/site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /opt/miniconda3/lib/python3.12/site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from pdfminer.six==20250506->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/miniconda3/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/miniconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# libraries for text cleaning\n",
    "%pip install nltk\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords as sw\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# library to read the PDF\n",
    "%pip install pdfplumber\n",
    "import pdfplumber\n",
    "\n",
    "# convert date string to datetime format\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should use this code\n",
    "# this is the text extracted from the first 3 pages extracted from the\n",
    "def raw_data_extraction(input_pdf):\n",
    "    # take the pdf as an input and return the whole pdf as text\n",
    "    raw_input=\"\"\n",
    "    with pdfplumber.open(input_pdf) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            raw_input+=(page.extract_text() or \"\") + \"\\n\"\n",
    "    return raw_input\n",
    "\n",
    "def article_extraction(raw_input):\n",
    "    article=\"\"\n",
    "    details=\"\"\n",
    "    article_idx=0\n",
    "    article_dict={}\n",
    "    details_dict={}\n",
    "    article_status=False # mark the start of the article\n",
    "    details_status=False\n",
    "    for row in raw_input.lower().split('\\n'):\n",
    "        if row=='full text':\n",
    "            article_status=True\n",
    "            article_idx+=1\n",
    "            continue\n",
    "        if article_status:\n",
    "            if row=='details':\n",
    "                article_status=False\n",
    "                details_status=True\n",
    "                article_dict[article_idx] = article\n",
    "                article=\"\"\n",
    "                continue\n",
    "            article+=row+\" \"\n",
    "        if details_status:\n",
    "            if row=='links':\n",
    "                details_status=False\n",
    "                details_dict[article_idx]=details\n",
    "                details=\"\"\n",
    "                continue\n",
    "            details+=row+'\\n'\n",
    "    return article_dict, details_dict\n",
    "\n",
    "def clean_article(article):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    desc = re.sub(r'\\([^)]*\\)', '', article)\n",
    "    sent_desc =sent_tokenize(desc)\n",
    "    sww=set(sw.words())\n",
    "    abstraction = []\n",
    "    for sent in sent_desc:\n",
    "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent.lower())\n",
    "        words = word_tokenize(tokens)\n",
    "        remaining_words = [word for word in words if word not in sww]\n",
    "        if remaining_words:\n",
    "            lemmatised_words=[]\n",
    "            for word in remaining_words:\n",
    "                lemmatised_word=lemmatizer.lemmatize(word)\n",
    "                lemmatised_words.append(lemmatised_word)\n",
    "            abstraction.append(lemmatised_words)\n",
    "    final_doc=\"\"\n",
    "    for sent in abstraction[:-1]:\n",
    "        for word in sent:\n",
    "            final_doc+=word+\" \"\n",
    "    return final_doc # remove the name of the writer\n",
    "\n",
    "    # above for loop why is it --> for sent in abstraction[:-1]???? we lose the last sentence here right?\n",
    "\n",
    "def clean_detail(detail):\n",
    "    year=\"\"\n",
    "    date=\"\"\n",
    "    for row in detail.split('\\n'):\n",
    "        if row.split(\":\")[0]==\"publication year\":\n",
    "            year = str(row.split(\":\")[1])\n",
    "        if row.split(\":\")[0]==\"publication date\":\n",
    "            raw_date=row.split(\":\")[1].split(\",\")[0]\n",
    "            full_raw_date= raw_date + year\n",
    "            #cleaned_date=raw_date.strip()\n",
    "            #cleaned_date = re.sub(r\"(\\d{2})\\s+(\\d{2})\", r\"\\1\\2\", cleaned_date)\n",
    "            format_date= \" %b %d %Y\"           \n",
    "            date = datetime.strptime(full_raw_date, format_date)\n",
    "            return date.date()\n",
    "    \n",
    "\n",
    "raw_input1=raw_data_extraction('3pages.pdf')\n",
    "article1, details1=article_extraction(raw_input1)\n",
    "doc1=clean_article(article1[1])\n",
    "date1=clean_detail(details1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1=clean_detail(details1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservative economist erwin john \"e.j.\" antoni sometimes jokes on social media that the \"l\" in bls is silent. president trump this week tapped antoni to run the bureau of labor statistics, the agency whose data and methodologies he has long criticized, especially when it produces numbers that trump doesn't like. antoni recently proposed suspending the monthly jobs report, one of the most important data releases for the economy and markets. on tuesday, a white house official noted that antoni made the comment before he knew he was going to be chosen and that his comments don't reflect official bls policy. if confirmed by the senate, antoni would run a 141-year-old agency staffed by around 2,000 economists, statisticians and other officials. the bls has a long record of independence and nonpartisanship that economists and investors say is critical to the credibility of u.s. economic data. according to a commencement program from northern illinois university, antoni earned a master's and ph.d. in economics from that school in 2018 and 2020, respectively, and a bachelor of arts from st. charles borromeo seminary. antoni's linkedin profile says he attended lansdale catholic high school outside philadelphia from 2002 to 2006. according to the profile, antoni went to work in 2021 as an economist at the texas public policy foundation, a conservative think tank in austin that has sued the federal government to overturn climate-change regulations. the following year, he joined the conservative heritage foundation as a research fellow studying regional economics. he is now the foundation's chief economist and an adviser to the committee to unleash prosperity, a group of conservative economic commentators. past bls commissioners have had extensive research experience, and many have climbed the agency's ranks. antoni doesn't fit that profile. he doesn't appear to have published any formal academic research since his dissertation, according to queries of national bureau of economic research working papers and google scholar. much of his commentary on the heritage website praises trump's policies and economic record. he frequently posts on x and appears on right-wing podcasts such as former trump adviser steve bannon's \"war room,\" where he criticized the economy under president joe biden and lauds trump's economy. the heritage foundation declined to make antoni available for an interview and didn't respond to questions about his background. while antoni's linkedin profile says his ph.d. concentration was in fiscal policy and labor economics, the dissertation on file at northern illinois principally analyzes fiscal policy: the \"crowding out\" effect of deficit spending, the effect of state taxes on domestic migration, and what he called the irrelevance of credit ratings on municipal debt yields. according to google scholar, antoni's paper has earned one citation, by the texas public policy foundation in 2021, while he worked there. publications by erika mcentarfer -- the bls commissioner whom trump ousted on aug. 1, midway through her term -- have been cited 1,327 times. trump fired mcentarfer after a report showing significant downward revisions to prior months' job growth. \"it's not a matter of making the numbers look good, it's a matter of them being accurate,\" antoni said on bannon's aug. 1 show after mcentarfer's dismissal. \"the models and the methodologies need to be revised.\" antoni told fox news digital in an interview on aug. 4 that \"bls should suspend issuing the monthly job reports\" until it changed methodologies. such a move would be unprecedented, leaving the public and markets without a vital source of information on the economy's health. his commentary on the data has been partisan. during biden's final year in office, antoni argued the consumer-price index was understating inflation. in july 2024, he said its monthly rent data was \"stale\" and the real costs wouldn't show up until after the election. in fact, while bls data showed rents rising more slowly than private data in 2021-22, by 2024 the reverse was true: it showed rents up 5.2% that june from a year earlier, compared with 3.2% for zillow. on bannon's aug. 1 show, he wrongly said that biden had removed mcentarfer's predecessor, bill beach, whom trump appointed in 2019. in fact, beach remained in the job until his four-year term expired in 2023, something beach has said he was \"very grateful\" for. conservatives praised trump's choice. \"ej antoni is one of the sharpest economic minds in the nation -- a fearless truth-teller who grasps that sound economics must serve the interests of american families, not globalist elites,\" heritage foundation president kevin roberts said in a statement. but several independent economists said he is unqualified. \"there are a lot of competent conservative economists that could do this job,\" said kyle pomerleau, a senior fellow at the right-of-center american enterprise institute, in a social-media post. \"e.j. is not one of them.\" sen. bill cassidy (r., la.), chairman of the senate committee on health, education, labor and pensions said in a statement, \"we need a bls commissioner committed to producing accurate, unbiased economic information to the american people. chairman cassidy looks forward to meeting with dr. antoni to discuss how he will accomplish this.\" --- move could backfire if data are seen as unreliable installing a partisan as the commissioner of the bureau of labor statistics could prove counterproductive to president trump, said renaissance macro research economist neil dutta. if e.j. antoni prods the agency into producing statistics that flatter the administration, the data would either not be seen as credible or would likely delay interest-rate cuts by the federal reserve. \"neither of those scenarios actually works to the president's benefit,\" dutta said. bill beach, the bls commissioner appointed during trump's first term, said antoni's suggestion of suspending publication of the monthly jobs report would be a bad idea for shoring up trust in the data. \"i think you fix these things while you're also publishing and make that extremely transparent,\" beach said. antoni, he said, \"has to be the buffer between the white house and the agency.\" by paul kiernan \n"
     ]
    }
   ],
   "source": [
    "print(article1[1]) # 1 article 1, 2 article 2, 3 article 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2025, 8, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added .strip() here to remove spaces -  can try both date_object codes\n",
    "\n",
    "#from datetime import datetime\n",
    "#format_date= \"%b %d, %Y\"\n",
    "#date_object = datetime.strptime(date1, format_date)\n",
    "#date_object = datetime.strptime(row.split(\":\")[1].strip(), format_date)\n",
    "#print(date_object.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To find the unique words and count for the 222 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input222 = raw_data_extraction('ProQuestDocuments-2025-08-16_1_fiscalpolicy_last12.pdf')\n",
    "article222_dict, details222_dict = article_extraction(raw_input222)\n",
    "\n",
    "#for key in article222_dict.keys():\n",
    " #   cleaned_article222 = clean_article(article222_dict[key])\n",
    " #   print(cleaned_article222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "\n",
    "for key, article in article222_dict.items():\n",
    "    cleaned_article222 = clean_article(article)\n",
    "    tokens = cleaned_article222.split()   # split cleaned string into words\n",
    "    all_tokens.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 11008\n",
      "Top n... words: [('tax', 1199), ('trump', 1099), ('year', 918), ('cut', 698), ('republican', 556), ('house', 503), ('president', 480), ('tariff', 454), ('spending', 451), ('bill', 434), ('policy', 415), ('government', 414), ('budget', 371), ('china', 356), ('rate', 352), ('company', 352), ('fiscal', 337), ('1', 328), ('economic', 321), ('federal', 317), ('economy', 310), ('state', 309), ('billion', 298), ('deficit', 287), ('market', 284), ('plan', 280), ('price', 279), ('growth', 273), ('increase', 268), ('administration', 264), ('million', 261), ('senate', 259), ('trillion', 251), ('2', 242), ('country', 235), ('cost', 233), ('trade', 233), ('month', 229), ('higher', 227), ('debt', 223), ('000', 219), ('time', 218), ('inflation', 218), ('make', 216), ('term', 209), ('investor', 204), ('5', 202), ('program', 201), ('bank', 201), ('4', 198), ('official', 196), ('day', 196), ('change', 195), ('week', 194), ('income', 193), ('dollar', 193), ('security', 193), ('money', 189), ('economist', 184), ('including', 184), ('democrat', 180), ('3', 177), ('business', 177), ('vote', 176), ('american', 175), ('congress', 169), ('interest', 168), ('party', 168), ('treasury', 164), ('work', 163), ('recent', 162), ('back', 162), ('high', 160), ('pay', 160), ('biden', 159), ('10', 159), ('investment', 157), ('white', 154), ('medicaid', 154), ('office', 152), ('law', 152), ('committee', 151), ('leader', 151), ('harris', 150), ('data', 148), ('energy', 145), ('agency', 144), ('lower', 142), ('stock', 141), ('yield', 140), ('top', 140), ('social', 137), ('group', 136), ('national', 136), ('benefit', 136), ('part', 136), ('bond', 136), ('chinese', 135), ('move', 132), ('decade', 132)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words:\", len(word_counts))\n",
    "print(\"Top n... words:\", word_counts.most_common(100))\n",
    "#print(\"Count for 'tax':\", word_counts['tax'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "bigram_counts = Counter()\n",
    "\n",
    "for article in article222_dict.values():\n",
    "    cleaned = clean_article(article)     # your existing cleaner\n",
    "    tokens = cleaned.split()             # turn string into list of words\n",
    "    bigrams = [\" \".join(bg) for bg in ngrams(tokens, 2)]\n",
    "    bigram_counts.update(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tax cut: 335\n",
      "white house: 146\n",
      "president trump: 125\n",
      "spending cut: 120\n",
      "interest rate: 109\n",
      "trump administration: 99\n",
      "fiscal year: 86\n",
      "budget deficit: 86\n",
      "wall street: 78\n",
      "social security: 75\n",
      "donald trump: 72\n",
      "economic growth: 69\n",
      "tax credit: 67\n",
      "enlarge image: 62\n",
      "house republican: 60\n",
      "5 trillion: 59\n",
      "state local: 52\n",
      "central bank: 51\n",
      "tax rate: 51\n",
      "vice president: 50\n",
      "10 year: 49\n",
      "2017 tax: 46\n",
      "chief executive: 45\n",
      "real estate: 45\n",
      "president elect: 45\n",
      "federal reserve: 44\n",
      "tax increase: 44\n",
      "budget committee: 43\n",
      "local tax: 41\n",
      "federal government: 39\n",
      "biden administration: 38\n",
      "senate republican: 36\n",
      "treasury yield: 36\n",
      "year ago: 35\n",
      "p 500: 35\n",
      "tax tip: 35\n",
      "expiring tax: 35\n",
      "local government: 35\n",
      "street journal: 34\n",
      "trump tariff: 34\n",
      "income tax: 34\n",
      "security benefit: 34\n",
      "house senate: 33\n",
      "percentage point: 33\n",
      "president biden: 33\n",
      "1 5: 32\n",
      "4 trillion: 32\n",
      "clean energy: 32\n",
      "trump term: 31\n",
      "former president: 31\n",
      "long term: 31\n",
      "tax spending: 31\n",
      "border security: 31\n",
      "tax bill: 31\n",
      "billion dollar: 31\n",
      "fiscal policy: 30\n",
      "10 000: 30\n",
      "longer term: 30\n",
      "tax deduction: 30\n",
      "kamala harris: 30\n",
      "national security: 29\n",
      "federal budget: 29\n",
      "rate cut: 28\n",
      "prime minister: 28\n",
      "short term: 28\n",
      "trump tax: 28\n",
      "low income: 28\n",
      "social medium: 27\n",
      "gross domestic: 27\n",
      "domestic product: 27\n",
      "barrel day: 27\n",
      "stock market: 27\n",
      "elon musk: 26\n",
      "borrowing cost: 26\n",
      "cut spending: 26\n",
      "respond request: 26\n",
      "request comment: 26\n",
      "cut expire: 26\n",
      "energy tax: 26\n",
      "recent year: 26\n",
      "cut tax: 25\n",
      "elect donald: 25\n",
      "year earlier: 24\n",
      "familiar matter: 24\n",
      "big beautiful: 24\n",
      "military spending: 24\n",
      "2 trillion: 24\n",
      "overtime pay: 24\n",
      "health insurance: 24\n",
      "work requirement: 24\n",
      "mansion tax: 24\n",
      "trade policy: 23\n",
      "2 5: 23\n",
      "supply chain: 23\n",
      "spending bill: 23\n",
      "income household: 23\n",
      "trillion spending: 23\n",
      "higher tariff: 22\n",
      "european union: 22\n",
      "covid 19: 22\n"
     ]
    }
   ],
   "source": [
    "top_100_bigrams = bigram_counts.most_common(100)\n",
    "\n",
    "for phrase, count in top_100_bigrams:\n",
    "    print(f\"{phrase}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the set of fiscal policy related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "product-gdp', 'tax rates', 'economists', 'government spending', 'investments', 'voters', 'inflation', 'legislators', 'taxes',\n",
    "                'federal reserve monetory policy', 'investors', 'presidential elections', 'congressional committees', 'borrowing', 'central banks', 'immigration policy', \n",
    "                'international trade', 'economic conditions', 'legislation', 'american dollar', 'social security', 'dow jones averages', 'pandemics', 'treasuries', 'costs',\n",
    "                'funding', 'manufacturing', 'political leadership', 'clean technology', 'national debt', 'national security', 'stocks', 'exports', 'immigrants',\n",
    "                'leadership', 'medicare', 'political parties', 'prime ministers', 'scandals', 'tax credits', 'trade policy', 'border security', 'consumer price index',\n",
    "                'defense spending', 'economic impact', 'economic policy', 'employees', 'federal legislation', 'households', 'immigration', 'international relations', 'low income groups',\n",
    "                'prices', 'social networks', 'tax legislation', 'trade disputes', 'budgets', 'consumers', 'corporate profits', 'currency', 'deportation', \n",
    "                'infrastructure', 'international economic relations', 'nominations', 'nvidia corp', 'polls & surveys', 'provisions', 'recessions', 'securities markets',\n",
    "                'trade relations', 'bond markets', 'cities', 'consumptions', 'cost control', 'decision making', 'deficit financing', 'economic crisis', 'economic development enforcement', \n",
    "                'factories', 'federal funding', 'global economy', 'government bonds'# the set of recurring words in these articles related to fiscal policy\n",
    "fiscal_terms=['tariffs', 'tax cuts', 'budget deficits', 'presidents', 'fiscal policy', 'political campaigns', 'federal budget', 'economic growth', 'interest rates',\n",
    "                'tax increases', 'gross domestic , 'income taxes', 'international relations-us', 'profits', 'stock exchanges', 'supply chains', 'tax refunds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group all terms into groups\n",
    "terms_categories={'tax': ['tax cuts','tax increases', 'tax rates', 'tax credits', 'taxes', 'tax legislation', 'income taxes', 'tax refunds', 'corporate profits', 'profits'],\n",
    "                'tariffs_trade': ['tariffs', 'trade policy', 'trade disputes','international trade','trade relations', 'exports','supply chains', 'border security', 'international relations-us', 'international relations', 'immigration policy', 'immigrants', 'immigration', 'deportation'],\n",
    "                'budget_debt': ['budget deficits', 'federal budget', 'budgets', 'deficit financing', 'national debt', 'treasuries', 'borrowing', 'federal funding', 'funding', 'cost control', 'costs', 'investments', 'provisions'],\n",
    "                'spending_social_program': ['government spending', 'defense spending', 'infrastructure', 'social security', 'medicare', 'federal legislation', 'households', 'low income groups', 'national security', 'pandemics', 'manufacturing'],\n",
    "                'monetary_financial_policy': ['government bonds', 'bond markets', 'federal reserve monetory policy', 'central banks', 'stocks', 'stock exchanges', 'dow jones averages','american dollar', 'currency', 'securities markets', 'fiscal policy', 'interest rates', 'inflation', 'consumer price index', 'prices', 'recessions', 'enforcement'],\n",
    "                'economy': ['economists', 'economic growth', 'economic conditions', 'economic policy', 'economic impact', 'international economic relations', 'global economy', 'economic crisis', 'economic development', 'gross domestic product-gdp', 'economic development enforcement'],\n",
    "                'other_context': ['presidents', 'investors', 'political campaigns', 'political leadership', 'political parties', 'legislation','congressional committees','prime ministers','scandals','nominations', 'clean technology', 'cities', 'voters', 'legislators', 'leadership', 'consumers', 'social networks', 'presidential elections', 'nvidia corp', 'polls & surveys', 'consumptions', 'decision making','employees', 'factories']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GONNA DELETE IT LATER\n",
    "# check for missing words not in the dictionary -  ensure it could not print anything\n",
    "#check_list=[]\n",
    "#for list in terms_groups.values():\n",
    "    #for word in list:\n",
    "        #check_list.append(word)\n",
    "\n",
    "#for word in fiscal_terms:\n",
    "    #if word not in check_list:\n",
    "        #print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match the terms and categories with the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this for the list of fiscal_terms (not categories)\n",
    "# lemmatizing the set of fiscal words so they match the lemmatized articles\n",
    "def lemmatize_string(string, lemmatizer):\n",
    "    return \" \".join(lemmatizer.lemmatize(w) for w in string.split())\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "fiscal_terms_lemmatized = [lemmatize_string(term, lemmatizer) for term in fiscal_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'president': 5,\n",
       " 'fiscal policy': 2,\n",
       " 'interest rate': 1,\n",
       " 'economist': 8,\n",
       " 'inflation': 1,\n",
       " 'tax': 1,\n",
       " 'investor': 1,\n",
       " 'cost': 1,\n",
       " 'consumer price index': 1,\n",
       " 'price': 1,\n",
       " 'consumer': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match the fiscal terms to the document\n",
    "def count_matching_words(terms_list, document):\n",
    "    terms_count={}\n",
    "    for term in terms_list:\n",
    "        term_count=document.count(term)\n",
    "        if term_count>0:\n",
    "            terms_count[term]=term_count\n",
    "    return terms_count\n",
    "\n",
    "count_matching_words(fiscal_terms_lemmatized,doc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this for the categoires of fiscal_terms (not list)\n",
    "# lemmatizing the set of fiscal words so they match the lemmatized articles\n",
    "def lemmatize_categories(categories, lemmatizer):\n",
    "    lemmatized = {}\n",
    "    for key, terms in categories.items():\n",
    "        lemmatized[key] = [lemmatize_string(term, lemmatizer) for term in terms]\n",
    "    return lemmatized\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "terms_categories_lemmatized = lemmatize_categories(terms_categories, lemmatizer) # keep this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'tax': 1,\n",
       "  'budget_debt': 1,\n",
       "  'monetary_financial_policy': 6,\n",
       "  'economy': 8,\n",
       "  'other_context': 7},\n",
       " True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match the fiscal term groups to the document\n",
    "def count_matching_category(terms_categories, document):\n",
    "    # this function will count number of terms in fiscal policy categories and classified whether a new is fical_policy related or not\n",
    "    terms_list=[word for category in terms_categories.values() for word in category]\n",
    "    terms_count={}\n",
    "    is_fiscal=False\n",
    "    for term in terms_list:\n",
    "        term_count=document.count(term)\n",
    "        if term_count>0:\n",
    "            for key in terms_categories.keys():\n",
    "                if term in terms_categories[key]:\n",
    "                    terms_count[key]=terms_count.get(key,0)+term_count\n",
    "    if len(terms_count.keys())>=3:\n",
    "        is_fiscal=True\n",
    "    return terms_count, is_fiscal # return the boolean values for terms_count.keys()>=3\n",
    "\n",
    "terms_count, is_fiscal=count_matching_category(terms_categories_lemmatized,doc1)\n",
    "terms_count, is_fiscal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "- We need at least one word related to debt, tax and tariff\n",
    "- Doing a dataframe:\n",
    "    + article title\n",
    "    + date\n",
    "    + is_fiscal: 0 or 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the dataframe to store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names =['article_id', 'date', 'is_fiscal_article']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do a function to classify the fiscal news -> #done\n",
    "- Group all the function into a pipeline\n",
    "- Add the boolean values to count the terms_categories groups.\n",
    "- Run all 222 articles to put in the dataframe \n",
    "- Create the calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>is_fiscal_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-08-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id        date  is_fiscal_article\n",
       "1           1  2025-08-14                  1\n",
       "2           2  2025-08-13                  1\n",
       "3           3  2025-08-13                  1\n",
       "4           4  2025-08-09                  1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def full_pipeline(input_pdf):\n",
    "    raw_input=raw_data_extraction(input_pdf)\n",
    "    article_dict, details_dict = article_extraction(raw_input)\n",
    "    column_names =['article_id', 'date', 'is_fiscal_article']\n",
    "    fiscal_articles_df=pd.DataFrame(columns=column_names)\n",
    "    for key in article_dict.keys():\n",
    "        cleaned_article=clean_article(article_dict[key])\n",
    "        #print(cleaned_article)\n",
    "        date=clean_detail(details_dict[key])\n",
    "        #print(date)\n",
    "        number_matching_words = count_matching_words(terms_categories_lemmatized, cleaned_article)\n",
    "        number_matching_terms, is_fiscal=count_matching_category(terms_categories_lemmatized,cleaned_article)\n",
    "        if is_fiscal:\n",
    "            fiscal_articles_df.loc[key]=[key, date, 1]\n",
    "    return fiscal_articles_df\n",
    "\n",
    "full_pipeline('21pages.pdf')    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
