{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/henrytran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/henrytran/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: feedparser in /Users/henrytran/Library/Python/3.9/lib/python/site-packages (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in /Users/henrytran/Library/Python/3.9/lib/python/site-packages (from feedparser) (1.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# libraries for text cleaning\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as sw\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# library to read the PDF\n",
    "%pip install feedparser\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: feedparser in /Users/henrytran/Library/Python/3.9/lib/python/site-packages (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in /Users/henrytran/Library/Python/3.9/lib/python/site-packages (from feedparser) (1.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# library to read the PDF\n",
    "%pip install feedparser\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 of 222\n",
      "U.S. News: BLS Pick Has Forged Partisan Path ---\n",
      "Nominee has praised Trump's policies but lacks the\n",
      "experience of his predecessors\n",
      "Kiernan, Paul . Kiernan, Paul.\n",
      "ProQuest document link\n",
      "FULL TEXT\n",
      "Conservative economist Erwin John \"E.J.\" Antoni sometimes jokes on social media that the \"L\" in BLS is silent.\n",
      "President Trump this week tapped Antoni to run the Bureau of Labor Statistics, the agency whose data and\n",
      "methodologies he has long criticized, especially when it produces numbers that Trump doesn't like. Antoni recently\n",
      "proposed suspending the monthly jobs report, one of the most important data releases for the economy and\n",
      "markets. On Tuesday, a White House official noted that Antoni made the comment before he knew he was going to be\n",
      "chosen and that his comments don't reflect official BLS policy.\n",
      "If confirmed by the Senate, Antoni would run a 141-year-old agency staffed by around 2,000 economists, statisticians\n",
      "and other officials. The BLS has a long record of independence and nonpartisanship that economists and investors\n",
      "say is critical to the credibility of U.S. economic data.\n",
      "According to a commencement program from Northern Illinois University, Antoni earned a master's and Ph.D. in\n",
      "economics from that school in 2018 and 2020, respectively, and a bachelor of arts from St. Charles Borromeo\n",
      "Seminary. Antoni's LinkedIn profile says he attended Lansdale Catholic High School outside Philadelphia from 2002\n",
      "to 2006.\n",
      "According to the profile, Antoni went to work in 2021 as an economist at the Texas Public Policy Foundation, a\n",
      "conservative think tank in Austin that has sued the federal government to overturn climate-change regulations. The\n",
      "following year, he joined the conservative Heritage Foundation as a research fellow studying regional economics. He\n",
      "is now the foundation's chief economist and an adviser to the Committee to Unleash Prosperity, a group of\n",
      "conservative economic commentators.\n",
      "Past BLS commissioners have had extensive research experience, and many have climbed the agency's ranks. Antoni\n",
      "doesn't fit that profile. He doesn't appear to have published any formal academic research since his dissertation,\n",
      "according to queries of National Bureau of Economic Research working papers and Google Scholar. Much of his\n",
      "commentary on the Heritage website praises Trump's policies and economic record. He frequently posts on X and\n",
      "appears on right-wing podcasts such as former Trump adviser Steve Bannon's \"War Room,\" where he criticized the\n",
      "economy under President Joe Biden and lauds Trump's economy.\n",
      "The Heritage Foundation declined to make Antoni available for an interview and didn't respond to questions about\n",
      "his background.\n",
      "While Antoni's LinkedIn profile says his Ph.D. concentration was in fiscal policy and labor economics, the dissertation\n",
      "on file at Northern Illinois principally analyzes fiscal policy: the \"crowding out\" effect of deficit spending, the effect of\n",
      "state taxes on domestic migration, and what he called the irrelevance of credit ratings on municipal debt yields.\n",
      "According to Google Scholar, Antoni's paper has earned one citation, by the Texas Public Policy Foundation in 2021,\n",
      "while he worked there. Publications by Erika McEntarfer -- the BLS commissioner whom Trump ousted on Aug. 1,\n",
      "midway through her term -- have been cited 1,327 times.\n",
      "Trump fired McEntarfer after a report showing significant downward revisions to prior months' job growth.\n",
      "\"It's not a matter of making the numbers look good, it's a matter of them being accurate,\" Antoni said on Bannon's\n",
      "Aug. 1 show after McEntarfer's dismissal. \"The models and the methodologies need to be revised.\"\n",
      "Antoni told Fox News Digital in an interview on Aug. 4 that \"BLS should suspend issuing the monthly job reports\" until\n",
      "it changed methodologies. Such a move would be unprecedented, leaving the public and markets without a vital\n",
      "source of information on the economy's health.\n",
      "His commentary on the data has been partisan. During Biden's final year in office, Antoni argued the consumer-price\n",
      "index was understating inflation. In July 2024, he said its monthly rent data was \"stale\" and the real costs wouldn't\n",
      "show up until after the election. In fact, while BLS data showed rents rising more slowly than private data in 2021-22,\n",
      "by 2024 the reverse was true: It showed rents up 5.2% that June from a year earlier, compared with 3.2% for Zillow.\n",
      "On Bannon's Aug. 1 show, he wrongly said that Biden had removed McEntarfer's predecessor, Bill Beach, whom\n",
      "Trump appointed in 2019. In fact, Beach remained in the job until his four-year term expired in 2023, something Beach\n",
      "has said he was \"very grateful\" for.\n",
      "Conservatives praised Trump's choice. \"EJ Antoni is one of the sharpest economic minds in the nation -- a fearless\n",
      "truth-teller who grasps that sound economics must serve the interests of American families, not globalist elites,\"\n",
      "Heritage Foundation President Kevin Roberts said in a statement.\n",
      "But several independent economists said he is unqualified. \"There are a lot of competent conservative economists\n",
      "that could do this job,\" said Kyle Pomerleau, a senior fellow at the right-of-center American Enterprise Institute, in a\n",
      "social-media post. \"E.J. is not one of them.\"\n",
      "Sen. Bill Cassidy (R., La.), chairman of the Senate Committee on Health, Education, Labor and Pensions said in a\n",
      "statement, \"We need a BLS Commissioner committed to producing accurate, unbiased economic information to the\n",
      "American people. Chairman Cassidy looks forward to meeting with Dr. Antoni to discuss how he will accomplish this.\"\n",
      "---\n",
      "Move Could Backfire If Data Are Seen as Unreliable\n",
      "Installing a partisan as the commissioner of the Bureau of Labor Statistics could prove counterproductive to\n",
      "President Trump, said Renaissance Macro Research economist Neil Dutta.\n",
      "If E.J. Antoni prods the agency into producing statistics that flatter the administration, the data would either not be\n",
      "seen as credible or would likely delay interest-rate cuts by the Federal Reserve.\n",
      "\"Neither of those scenarios actually works to the president's benefit,\" Dutta said.\n",
      "Bill Beach, the BLS commissioner appointed during Trump's first term, said Antoni's suggestion of suspending\n",
      "publication of the monthly jobs report would be a bad idea for shoring up trust in the data.\n",
      "\"I think you fix these things while you're also publishing and make that extremely transparent,\" Beach said. Antoni, he\n",
      "said, \"has to be the buffer between the White House and the agency.\"\n",
      "By Paul Kiernan\n",
      "DETAILS\n",
      "Subject: Conservatism; Fiscal policy; Presidents; Economists; Public policy; Economics;\n",
      "Congressional committees\n",
      "Business indexing term: Subject: Fiscal policy Economists Economics; Corporation: LinkedIn Corp; Industry:\n",
      "92111 : Executive Offices 92112 : Legislative Bodies\n",
      "Location: Texas; United States--US\n",
      "People: Trump, Donald J; Antoni, Erwin John III\n",
      "Company / organization: Name: LinkedIn Corp; NAICS: 518210; Name: Bureau of Labor Statistics; NAICS: 921110,\n",
      "923110; Name: Texas Public Policy Foundation; NAICS: 813211; Name: Heritage\n",
      "Foundation-Washington DC; NAICS: 541720\n",
      "Classification: 92111: Executive Offices; 92112: Legislative Bodies\n",
      "Publication title: Wall Street Journal, Eastern edition; New York, N.Y.\n",
      "First page: A2\n",
      "Publication year: 2025\n",
      "Publication date: Aug 14, 2025\n",
      "Publisher: Dow Jones &Company Inc.\n",
      "Place of publication: New York, N.Y.\n",
      "Country of publication: United States\n",
      "Publication subject: Business And Economics--Banking And Finance\n",
      "ISSN: 00999660\n",
      "Source type: Newspaper\n",
      "Language of publication: English\n",
      "Document type: News\n",
      "ProQuest document ID: 3239236707\n",
      "Document URL: https://www.proquest.com/newspapers/u-s-news-bls-pick-has-forged-partisan-\n",
      "path/docview/3239236707/se-2?accountid=14681\n",
      "Copyright: Copyright 2025 Dow Jones &Company, Inc. All Rights Reserved.\n",
      "Full text availability: This publication may be subject to restrictions within certain markets, including\n",
      "corporations, non-profits, government institutions, and public libraries. In those cases\n",
      "records will be visible to users, but not full text.\n",
      "Last updated: 2025-08-14\n",
      "Database: ProQuest Central\n",
      "LINKS\n",
      "Document 2 of 222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We should use this code\n",
    "# this is the text extracted from the first 5 pages extracted from the\n",
    "def raw_data_extraction(input_pdf):\n",
    "    # take the pdf as an input and return the whole pdf as text\n",
    "    raw_input=\"\"\n",
    "    with pdfplumber.open(input_pdf) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            raw_input+=(page.extract_text() or \"\") + \"\\n\"\n",
    "    return raw_input\n",
    "\n",
    "def article_extraction(raw_input):\n",
    "    #all_news = re.sub(r'\\([^)]*\\)', '', raw_input)\n",
    "    #sww=set(sw.words())\n",
    "    article=\"\"\n",
    "    article_status=False # mark the start of the article\n",
    "    for row in raw_input.lower().split('\\n'):\n",
    "        if row=='full text':\n",
    "            article_status=True\n",
    "            continue\n",
    "        if article_status:\n",
    "            if row=='details':\n",
    "                article_status=False\n",
    "                continue\n",
    "            article+=row+\" \"\n",
    "        #tokens = re.sub(r\"[^a-z0-9]+\", \" \", row)\n",
    "        #words = word_tokenize(tokens)\n",
    "    return article\n",
    "\n",
    "# temporaryly do not need this\n",
    "def clean_article(article):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    desc = re.sub(r'\\([^)]*\\)', '', article)\n",
    "    sent_desc =sent_tokenize(desc)\n",
    "    sww=set(sw.words())\n",
    "    abstraction = []\n",
    "    for sent in sent_desc:\n",
    "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent.lower())\n",
    "        words = word_tokenize(tokens)\n",
    "        remaining_words = [word for word in words if word not in sww]\n",
    "        if remaining_words:\n",
    "            lemmatised_words=[]\n",
    "            for word in remaining_words:\n",
    "                lemmatised_word=lemmatizer.lemmatize(word)\n",
    "                lemmatised_words.append(lemmatised_word)\n",
    "            abstraction.append(lemmatised_words)\n",
    "    final_doc=\"\"\n",
    "    for sent in abstraction[:-1]:\n",
    "        for word in sent:\n",
    "            final_doc+=word+\" \"\n",
    "    return final_doc # remove the name of the writer\n",
    "\n",
    "raw_input1=raw_data_extraction('3pages.pdf')\n",
    "article1=article_extraction(raw_input1)\n",
    "doc1=clean_article(article1)\n",
    "print(raw_input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the set of fiscal policy related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the set of recurring words in these articles related to fiscal policy\n",
    "fiscal_terms=['tariffs', 'tax cuts', 'budget deficits', 'presidents', 'fiscal policy', 'political campaigns', 'federal budget', 'economic growth', 'interest rates',\n",
    "                'tax increases', 'gross domestic product-gdp', 'tax rates', 'economists', 'government spending', 'investments', 'voters', 'inflation', 'legislators', 'taxes',\n",
    "                'federal reserve monetory policy', 'investors', 'presidential elections', 'congressional committees', 'borrowing', 'central banks', 'immigration policy', \n",
    "                'international trade', 'economic conditions', 'legislation', 'american dollar', 'social security', 'dow jones averages', 'pandemics', 'treasuries', 'costs',\n",
    "                'funding', 'manufacturing', 'political leadership', 'clean technology', 'national debt', 'national security', 'stocks', 'exports', 'immigrants',\n",
    "                'leadership', 'medicare', 'political parties', 'prime ministers', 'scandals', 'tax credits', 'trade policy', 'border security', 'consumer price index',\n",
    "                'defense spending', 'economic impact', 'economic policy', 'employees', 'federal legislation', 'households', 'immigration', 'international relations', 'low income groups',\n",
    "                'prices', 'social networks', 'tax legislation', 'trade disputes', 'budgets', 'consumers', 'corporate profits', 'currency', 'deportation', \n",
    "                'infrastructure', 'international economic relations', 'nominations', 'nvidia corp', 'polls & surveys', 'provisions', 'recessions', 'securities markets',\n",
    "                'trade relations', 'bond markets', 'cities', 'consumptions', 'cost control', 'decision making', 'deficit financing', 'economic crisis', 'economic development'\n",
    "                'enforcement', 'factories', 'federal funding', 'global economy', 'government bonds', 'income taxes', 'international relations-us', 'profits', 'stock exchanges',\n",
    "                'supply chains', 'tax refunds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fiscal_terms={'tax': ['tax cuts','tax increases', 'tax rates', 'tax credits', 'taxes', 'tax legislation', 'income taxes', 'tax refunds'],\n",
    "                'government_social': ['government spending', 'government bonds', 'defense spending', 'infrastructure','social security', 'medicare', 'low income groups', 'households'],\n",
    "                'trade': ['tariffs']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match the terms with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiscal policy': 2, 'inflation': 1, 'consumer price index': 1}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match the fiscal terms to the document\n",
    "def count_matching_words(terms_list, document):\n",
    "    terms_count={}\n",
    "    for term in terms_list:\n",
    "        term_count=document.count(term)\n",
    "        if term_count >0:\n",
    "            terms_count[term]=term_count\n",
    "    return terms_count\n",
    "\n",
    "count_matching_words(fiscal_terms,doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This is the code for spaCy**\n",
    "\n",
    "Can consider using spaCy, because it is more powerful than NLTK, but also harder to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.17.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# use spaCy to match the words\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher # to match the words in the article with specified terms in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/thinc/compat.py\", line 99, in <module>\n",
      "    import h5py\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/h5py/__init__.py\", line 45, in <module>\n",
      "    from ._conv import register_converters as _register_converters, \\\n",
      "  File \"h5py/_conv.pyx\", line 1, in init h5py._conv\n",
      "  File \"h5py/h5r.pyx\", line 1, in init h5py.h5r\n",
      "  File \"h5py/h5p.pyx\", line 1, in init h5py.h5p\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the pretrained general english model (small size) - can considered any pretrained language model for finance or economic\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/spacy/__init__.py:52\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     29\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     36\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/spacy/util.py:484\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# load the pretrained general english model (small size) - can considered any pretrained language model for finance or economic\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the PhraseMatcher object to load the vocabulary and the fiscal policy terms\n",
    "article=nlp('') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
